looking:closeLock
synchronized (closeLock) {
    {
        {
            return this.closed.get();
        }
        if (coprocessorHost != null) {
            status.setStatus("Running coprocessor pre-close hooks");
            this.coprocessorHost.preClose(abort);
        }
        status.setStatus("Disabling compacts and flushes for region");
        boolean canFlush = true;
        synchronized (writestate) {
            // Disable compacting and flushing by background threads for this
            // region.
            canFlush = !writestate.readOnly;
            writestate.writesEnabled = false;
            LOG.debug("Closing {}, disabling compactions & flushes", this.getRegionInfo().getEncodedName());
            {
                synchronized (writestate) {
                    if (this.writestate.readOnly) {
                        // we should not wait for replayed flushed if we are read only (for example in case the
                        // region is a secondary replica).
                        return;
                    }
                    boolean interrupted = false;
                    try {
                        while (writestate.compacting.get() > 0 || writestate.flushing) {
                            LOG.debug("waiting for " + writestate.compacting + " compactions" + (writestate.flushing ? " & cache flush" : "") + " to complete for region " + this);
                            try {
                                writestate.wait();
                            } catch (InterruptedException iex) {
                                // essentially ignore and propagate the interrupt back up
                                LOG.warn("Interrupted while waiting in region {}", this);
                                interrupted = true;
                                break;
                            }
                        }
                    } finally {
                        if (interrupted) {
                            Thread.currentThread().interrupt();
                        }
                    }
                }
            }
        }
        // If we were not just flushing, is it worth doing a preflush...one
        // that will clear out of the bulk of the memstore before we put up
        // the close flag?
        if (!abort && worthPreFlushing() && canFlush) {
            status.setStatus("Pre-flushing region before close");
            LOG.info("Running close preflush of {}", this.getRegionInfo().getEncodedName());
            try {
                {
                    return internalFlushcache(stores.values(), status, false, FlushLifeCycleTracker.DUMMY);
                }
            } catch (IOException ioe) {
                // Failed to flush the region. Keep going.
                status.setStatus("Failed pre-flush " + this + "; " + ioe.getMessage());
            }
        }
        if (regionReplicationSink.isPresent()) {
            // stop replicating to secondary replicas
            // the open event marker can make secondary replicas refresh store files and catch up
            // everything, so here we just give up replicating later edits, to speed up the reopen process
            RegionReplicationSink sink = regionReplicationSink.get();
            sink.stop();
            try {
                regionReplicationSink.get().waitUntilStopped();
            } catch (InterruptedException e) {
                {
                    if (this.closing.get()) {
                        return (NotServingRegionException) new NotServingRegionException(getRegionInfo().getRegionNameAsString() + " is closing").initCause(t);
                    }
                    return (InterruptedIOException) new InterruptedIOException().initCause(t);
                }
            }
        }
        // Set the closing flag
        // From this point new arrivals at the region lock will get NSRE.
        this.closing.set(true);
        LOG.info("Closing region {}", this);
        // Acquire the close lock
        // The configuration parameter CLOSE_WAIT_ABORT is overloaded to enable both
        // the new regionserver abort condition and interrupts for running requests.
        // If CLOSE_WAIT_ABORT is not enabled there is no change from earlier behavior,
        // we will not attempt to interrupt threads servicing requests nor crash out
        // the regionserver if something remains stubborn.
        final boolean canAbort = conf.getBoolean(CLOSE_WAIT_ABORT, DEFAULT_CLOSE_WAIT_ABORT);
        boolean useTimedWait = false;
        if (timeoutForWriteLock != null && timeoutForWriteLock != Long.MAX_VALUE) {
            // convert legacy use of timeoutForWriteLock in seconds to new use in millis
            timeoutForWriteLock = TimeUnit.SECONDS.toMillis(timeoutForWriteLock);
            useTimedWait = true;
        } else if (canAbort) {
            timeoutForWriteLock = conf.getLong(CLOSE_WAIT_TIME, DEFAULT_CLOSE_WAIT_TIME);
            useTimedWait = true;
        }
        if (LOG.isDebugEnabled()) {
            LOG.debug((useTimedWait ? "Time limited wait" : "Waiting without time limit") + " for close lock on " + this);
        }
        final long closeWaitInterval = conf.getLong(CLOSE_WAIT_INTERVAL, DEFAULT_CLOSE_WAIT_INTERVAL);
        long elapsedWaitTime = 0;
        if (useTimedWait) {
            // Sanity check configuration
            long remainingWaitTime = timeoutForWriteLock;
            if (remainingWaitTime < closeWaitInterval) {
                LOG.warn("Time limit for close wait of " + timeoutForWriteLock + " ms is less than the configured lock acquisition wait interval " + closeWaitInterval + " ms, using wait interval as time limit");
                remainingWaitTime = closeWaitInterval;
            }
            boolean acquired = false;
            do {
                long start = EnvironmentEdgeManager.currentTime();
                try {
                    acquired = lock.writeLock().tryLock(Math.min(remainingWaitTime, closeWaitInterval), TimeUnit.MILLISECONDS);
                } catch (InterruptedException e) {
                    // Interrupted waiting for close lock. More likely the server is shutting down, not
                    // normal operation, so aborting upon interrupt while waiting on this lock would not
                    // provide much value. Throw an IOE (as IIOE) like we would in the case where we
                    // fail to acquire the lock.
                    String msg = "Interrupted while waiting for close lock on " + this;
                    LOG.warn(msg, e);
                    throw (InterruptedIOException) new InterruptedIOException(msg).initCause(e);
                }
                long elapsed = EnvironmentEdgeManager.currentTime() - start;
                elapsedWaitTime += elapsed;
                remainingWaitTime -= elapsed;
                if (canAbort && !acquired && remainingWaitTime > 0) {
                    // Before we loop to wait again, interrupt all region operations that might
                    // still be in progress, to encourage them to break out of waiting states or
                    // inner loops, throw an exception to clients, and release the read lock via
                    // endRegionOperation.
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Interrupting region operations after waiting for close lock for " + elapsedWaitTime + " ms on " + this + ", " + remainingWaitTime + " ms remaining");
                    }
                    {
                        for (Map.Entry<Thread, Boolean> entry : regionLockHolders.entrySet()) {
                            // An entry in this map will have a boolean value indicating if it is currently
                            // eligible for interrupt; if so, we should interrupt it.
                            if (entry.getValue().booleanValue()) {
                                entry.getKey().interrupt();
                            }
                        }
                    }
                }
            } while (!acquired && remainingWaitTime > 0);
            // If we fail to acquire the lock, trigger an abort if we can; otherwise throw an IOE
            // to let the caller know we could not proceed with the close.
            if (!acquired) {
                String msg = "Failed to acquire close lock on " + this + " after waiting " + elapsedWaitTime + " ms";
                LOG.error(msg);
                if (canAbort) {
                    // If we failed to acquire the write lock, abort the server
                    rsServices.abort(msg, null);
                }
                throw new IOException(msg);
            }
        } else {
            long start = EnvironmentEdgeManager.currentTime();
            lock.writeLock().lock();
            elapsedWaitTime = EnvironmentEdgeManager.currentTime() - start;
        }
        if (LOG.isDebugEnabled()) {
            LOG.debug("Acquired close lock on " + this + " after waiting " + elapsedWaitTime + " ms");
        }
        status.setStatus("Disabling writes for close");
        try {
            {
                return this.closed.get();
            }
            LOG.debug("Updates disabled for region " + this);
            // Don't flush the cache if we are aborting
            if (!abort && canFlush) {
                int failedfFlushCount = 0;
                int flushCount = 0;
                long tmp = 0;
                long remainingSize = this.memStoreSizing.getDataSize();
                while (remainingSize > 0) {
                    try {
                        internalFlushcache(status);
                        if (flushCount > 0) {
                            LOG.info("Running extra flush, " + flushCount + " (carrying snapshot?) " + this);
                        }
                        flushCount++;
                        tmp = this.memStoreSizing.getDataSize();
                        if (tmp >= remainingSize) {
                            failedfFlushCount++;
                        }
                        remainingSize = tmp;
                        if (failedfFlushCount > 5) {
                            // If we failed 5 times and are unable to clear memory, abort
                            // so we do not lose data
                            throw new DroppedSnapshotException("Failed clearing memory after " + flushCount + " attempts on region: " + Bytes.toStringBinary(getRegionInfo().getRegionName()));
                        }
                    } catch (IOException ioe) {
                        status.setStatus("Failed flush " + this + ", putting online again");
                        synchronized (writestate) {
                            writestate.writesEnabled = true;
                        }
                        // Have to throw to upper layers. I can't abort server from here.
                        throw ioe;
                    }
                }
            }
            Map<byte[], List<HStoreFile>> result = new TreeMap<>(Bytes.BYTES_COMPARATOR);
            if (!stores.isEmpty()) {
                // initialize the thread pool for closing stores in parallel.
                ThreadPoolExecutor storeCloserThreadPool = getStoreOpenAndCloseThreadPool("StoreCloser-" + getRegionInfo().getRegionNameAsString());
                CompletionService<Pair<byte[], Collection<HStoreFile>>> completionService = new ExecutorCompletionService<>(storeCloserThreadPool);
                // close each store in parallel
                for (HStore store : stores.values()) {
                    MemStoreSize mss = store.getFlushableSize();
                    if (!(abort || mss.getDataSize() == 0 || writestate.readOnly)) {
                        if (getRegionServerServices() != null) {
                            getRegionServerServices().abort("Assertion failed while closing store " + getRegionInfo().getRegionNameAsString() + " " + store + ". flushableSize expected=0, actual={" + mss + "}. Current memStoreSize=" + this.memStoreSizing.getMemStoreSize() + ". Maybe a coprocessor " + "operation failed and left the memstore in a partially updated state.", null);
                        }
                    }
                    completionService.submit(new Callable<Pair<byte[], Collection<HStoreFile>>>() {

                        @Override
                        public Pair<byte[], Collection<HStoreFile>> call() throws IOException {
                            return new Pair<>(store.getColumnFamilyDescriptor().getName(), store.close());
                        }
                    });
                }
                try {
                    for (int i = 0; i < stores.size(); i++) {
                        Future<Pair<byte[], Collection<HStoreFile>>> future = completionService.take();
                        Pair<byte[], Collection<HStoreFile>> storeFiles = future.get();
                        List<HStoreFile> familyFiles = result.get(storeFiles.getFirst());
                        if (familyFiles == null) {
                            familyFiles = new ArrayList<>();
                            {
                                TraceUtil.trace(() -> {
                                    {
                                        // If catalog region, do not impose resource constraints or block updates.
                                        if (this.getRegionInfo().isMetaRegion()) {
                                            return;
                                        }
                                        MemStoreSize mss = this.memStoreSizing.getMemStoreSize();
                                        if (mss.getHeapSize() + mss.getOffHeapSize() > this.blockingMemStoreSize) {
                                            blockedRequestsCount.increment();
                                            requestFlush();
                                            // Don't print current limit because it will vary too much. The message is used as a key
                                            // over in RetriesExhaustedWithDetailsException processing.
                                            final String regionName = this.getRegionInfo() == null ? "unknown" : this.getRegionInfo().getEncodedName();
                                            final String serverName = this.getRegionServerServices() == null ? "unknown" : (this.getRegionServerServices().getServerName() == null ? "unknown" : this.getRegionServerServices().getServerName().toString());
                                            RegionTooBusyException rtbe = new RegionTooBusyException("Over memstore limit=" + org.apache.hadoop.hbase.procedure2.util.StringUtils.humanSize(this.blockingMemStoreSize) + ", regionName=" + regionName + ", server=" + serverName);
                                            LOG.warn("Region is too busy due to exceeding memstore size limit.", rtbe);
                                            throw rtbe;
                                        }
                                    }
                                    // Do a rough check that we have resources to accept a write. The check is
                                    // 'rough' in that between the resource check and the call to obtain a
                                    // read lock, resources may run out. For now, the thought is that this
                                    // will be extremely rare; we'll deal with it when it happens.
                                    checkResources();
                                    startRegionOperation(Operation.PUT);
                                    try {
                                        {
                                            {
                                                {
                                                    return mutate(mutation, false);
                                                }
                                            }
                                        }
                                    } finally {
                                        closeRegionOperation(Operation.PUT);
                                    }
                                }, () -> createRegionSpan("Region.put"));
                            }
                        }
                        familyFiles.addAll(storeFiles.getSecond());
                    }
                } catch (InterruptedException e) {
                    {
                        if (this.closing.get()) {
                            return (NotServingRegionException) new NotServingRegionException(getRegionInfo().getRegionNameAsString() + " is closing").initCause(t);
                        }
                        return (InterruptedIOException) new InterruptedIOException().initCause(t);
                    }
                } catch (ExecutionException e) {
                    Throwable cause = e.getCause();
                    if (cause instanceof IOException) {
                        throw (IOException) cause;
                    }
                    throw new IOException(cause);
                } finally {
                    storeCloserThreadPool.shutdownNow();
                }
            }
            status.setStatus("Writing region close event to WAL");
            // Always write close marker to wal even for read only table. This is not a big problem as we
            // do not write any data into the region; it is just a meta edit in the WAL file.
            if (!abort && wal != null && getRegionServerServices() != null && RegionReplicaUtil.isDefaultReplica(getRegionInfo())) {
                {
                    Map<byte[], List<Path>> storeFiles = getStoreFiles();
                    RegionEventDescriptor regionEventDesc = ProtobufUtil.toRegionEventDescriptor(RegionEventDescriptor.EventType.REGION_CLOSE, getRegionInfo(), mvcc.getReadPoint(), getRegionServerServices().getServerName(), storeFiles);
                    // we do not care region close event at secondary replica side so just pass a null
                    // RegionReplicationSink
                    WALUtil.writeRegionEventMarker(wal, getReplicationScope(), getRegionInfo(), regionEventDesc, mvcc, null);
                    // Store SeqId in WAL FileSystem when a region closes
                    // checking region folder exists is due to many tests which delete the table folder while a
                    // table is still online
                    if (getWalFileSystem().exists(getWALRegionDir())) {
                        WALSplitUtil.writeRegionSequenceIdFile(getWalFileSystem(), getWALRegionDir(), mvcc.getReadPoint());
                    }
                }
            }
            this.closed.set(true);
            if (!canFlush) {
                decrMemStoreSize(this.memStoreSizing.getMemStoreSize());
            } else if (this.memStoreSizing.getDataSize() != 0) {
                LOG.error("Memstore data size is {} in region {}", this.memStoreSizing.getDataSize(), this);
            }
            if (coprocessorHost != null) {
                status.setStatus("Running coprocessor post-close hooks");
                this.coprocessorHost.postClose(abort);
            }
            if (this.metricsRegion != null) {
                this.metricsRegion.close();
            }
            if (this.metricsRegionWrapper != null) {
                {
                    {
                        {
                            {
                                {
                                    {
                                        {
                                            {
                                                {
                                                    {
                                                        return close(false);
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
            status.markComplete("Closed");
            LOG.info("Closed {}", this);
            return result;
        } finally {
            lock.writeLock().unlock();
        }
    }
}

synchronized (closeLock) {
    return doClose(abort, status);
}
possible Hot2
possible type Hot3_2
2
